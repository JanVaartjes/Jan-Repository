{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import json\n",
    "import time\n",
    "import dateutil.parser\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import redis\n",
    "import string\n",
    "import collections\n",
    "import re\n",
    "from IPython.core import display\n",
    "from Queue import Empty\n",
    "import itertools\n",
    "import datetime\n",
    "import twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Obtaining Twitter stream with API keys\n",
    "REALTIME_DATA = True\n",
    "twitter_stream = twitter.TwitterStream(auth=twitter.OAuth(\n",
    "token=\"304565489-yP9AWplxhW4MfspjsoONJ7dDcf6yWLUgc35Ha0Eh\",\n",
    "token_secret=\"serbdMPy4Vus6yASU2vVFPRCnntQkjijdMXMR6ukTbxH0\",\n",
    "consumer_key=\"L4bgPj1Gr9zDD4WVf3Z8z90Xf\",\n",
    "consumer_secret=\"ZKJgbXJ4fa1gcLg9CVjIx0VV7FDRtFgL8eXzZJxjnlx16Gx034\"))\n",
    "stream = twitter_stream.statuses.sample(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'appendonly': 'yes'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Redis Database setup\n",
    "!redis-cli ping &> /dev/null || (redis-server & disown)\n",
    "DB_NUMBER = 0\n",
    "r = redis.StrictRedis(host='localhost', port=6379, db=DB_NUMBER)\n",
    "r.config_set('appendonly', 'yes')\n",
    "r.config_get('appendonly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Obtaining variables of interest\n",
    "class Tweet(dict):\n",
    "    def __init__(self, raw_tweet):\n",
    "        super(Tweet, self).__init__(self)\n",
    "        if raw_tweet and 'delete' not in raw_tweet:\n",
    "            self['timestamp'] = dateutil.parser.parse(raw_tweet[u'created_at']\n",
    "                                ).replace(tzinfo=None).isoformat()\n",
    "            self['text'] = raw_tweet['text']\n",
    "            self['hashtags'] = [x['text'] for x in raw_tweet['entities']['hashtags']]\n",
    "            self['geo'] = raw_tweet['geo']['coordinates'] if raw_tweet['geo'] else None\n",
    "            self['id'] = raw_tweet['id']\n",
    "            self['screen_name'] = raw_tweet['user']['screen_name']\n",
    "            self['user_id'] = raw_tweet['user']['id'] \n",
    "            \n",
    "#Dumping empty tweets (i.e. deleted tweets)\n",
    "if REALTIME_DATA:\n",
    "    T = None\n",
    "    while not T:\n",
    "        T = Tweet(stream.next())\n",
    "else:\n",
    "    T = Tweet(json.load(open('one_tweet.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Language detection to remove unreadable tweets\n",
    "#Download stopwords packet\n",
    "try:\n",
    "    from nltk.corpus import stopwords # stopwords to detect language\n",
    "except LookupError:\n",
    "    !python -m nltk.downloader stopwords\n",
    "    from nltk.corpus import stopwords\n",
    "from nltk import wordpunct_tokenize # function to split up our words\n",
    "\n",
    "#Determine most likely language\n",
    "def get_likely_language(input_text):\n",
    "    input_text = input_text.lower()\n",
    "    input_words = wordpunct_tokenize(input_text)\n",
    " \n",
    "    likely_language = 'unknown'\n",
    "    likely_language_matches = 0\n",
    "    total_matches = 0\n",
    "    stopword_sets = dict([(lang, set(stopwords.words(lang))) \n",
    "                            for lang in stopwords._fileids])\n",
    "    \n",
    "    for language in np.random.permutation(stopwords._fileids):\n",
    "        language_matches = len(set(input_words) & stopword_sets[language])\n",
    "        total_matches += language_matches\n",
    "        if language_matches > likely_language_matches:\n",
    "            likely_language_matches = language_matches\n",
    "            likely_language = language\n",
    " \n",
    "    return (likely_language, likely_language_matches, total_matches)\n",
    "\n",
    "#Obtain raw Twitter data and load into objects\n",
    "SENTINEL = '$QUIT'\n",
    "def load_tweets(twitter_stream, out_q, time_limit):\n",
    "    end_time = time.time() + 60*time_limit\n",
    "    twiterator = itertools.chain.from_iterable(\n",
    "                    itertools.repeat(\n",
    "                    twitter_stream.statuses.sample()))\n",
    "    try:\n",
    "        for raw_tweet in twiterator:\n",
    "            T = Tweet(raw_tweet)\n",
    "            if T: \n",
    "                out_q.put(T)\n",
    "            if time.time() > end_time:\n",
    "                break\n",
    "    finally:\n",
    "        out_q.put(SENTINEL)\n",
    "        \n",
    "#Guess language, output only recognized tweets\n",
    "def get_language(in_q, out_q):\n",
    "    try:\n",
    "        while True:\n",
    "            try:\n",
    "                T = in_q.get(timeout=1)\n",
    "            except Empty:\n",
    "                continue\n",
    "            if T == SENTINEL:\n",
    "                break\n",
    "            elif T:\n",
    "                lang, matches, total_matches = get_likely_language(T['text'])\n",
    "                if lang and matches > 1:\n",
    "                    T['language'] = lang\n",
    "                    out_q.put(T)\n",
    "    finally:\n",
    "        out_q.put(SENTINEL)\n",
    "        in_q.put(SENTINEL)\n",
    "        \n",
    "#Take selected data from Tweet objects, only interested in hashtags and text\n",
    "def write_records(in_q, db):\n",
    "    try:\n",
    "        while True:\n",
    "            try:\n",
    "                T = in_q.get(timeout=1)\n",
    "            except Empty:\n",
    "                continue\n",
    "            if T == SENTINEL:\n",
    "                break\n",
    "            elif T and T.get('language'):\n",
    "                if T['hashtags']:\n",
    "                    db.rpush('hashtag_' + T['language'], *T['hashtags'])\n",
    "                if T['text']:\n",
    "                    db.rpush('text_' + T['language'], json.dumps(T['text']))\n",
    "    finally:\n",
    "        in_q.put(SENTINEL)\n",
    "\n",
    "TIME_LIMIT = 2 #in minutes\n",
    "# multiprocessing is imported as mp\n",
    "if REALTIME_DATA:\n",
    "    language_queue = mp.Queue()\n",
    "    write_queue = mp.Queue()\n",
    "    \n",
    "    # Stream worker\n",
    "    process_list = []\n",
    "    process_list.append(mp.Process(\n",
    "            target=load_tweets, \n",
    "            args=(twitter_stream, language_queue, TIME_LIMIT)))\n",
    "\n",
    "if REALTIME_DATA:\n",
    "    # Two language detection workers\n",
    "    for i in range(2):\n",
    "        process_list.append(mp.Process(\n",
    "            target=get_language, \n",
    "            args=(language_queue, write_queue)))\n",
    "    \n",
    "    # DB write worker\n",
    "    process_list.append(mp.Process(\n",
    "            target=write_records, \n",
    "            args=(write_queue, r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processes at: 2016-10-10 17:00:56.219928\n",
      "Processes will end at: 2016-10-10 17:02:56.220163\n",
      "Current time: 17:00:56\n",
      "Redis memory used: 6.23M\n",
      "Queue lengths: [0L, 0L]\n",
      "Processes alive? [True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "#Status checkers\n",
    "if REALTIME_DATA:\n",
    "    print \"Starting processes at: %s\" % datetime.datetime.now()\n",
    "    print \"Processes will end at: %s\" % (datetime.datetime.now() + datetime.timedelta(minutes=TIME_LIMIT))\n",
    "    for p in process_list:\n",
    "        p.start()\n",
    "else:\n",
    "    print \"Restart Kernel\"\n",
    "    \n",
    "def check_status():\n",
    "    print \"Current time:\", datetime.datetime.now().time().replace(microsecond=0)\n",
    "    print \"Redis memory used:\", r.info()['used_memory_peak_human']\n",
    "    if REALTIME_DATA:\n",
    "        print \"Queue lengths:\", [q.qsize() for q in language_queue, write_queue]\n",
    "        print \"Processes alive?\", [x.is_alive() for x in process_list]\n",
    "        \n",
    "check_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    danish: dkpol 2\n",
      "     dutch: win 6\n",
      "   english: aldubyoualways 4\n",
      "   finnish: kala 1\n",
      "    french: amiraf2016 2\n",
      "    german: gameinsight 10\n",
      " hungarian: tuitutil 2\n",
      "   italian: saiche 9\n",
      "    kazakh: \n",
      " norwegian: timylovestruck 5\n",
      "portuguese: umrei 20\n",
      "   russian: gameinsight 18\n",
      "   spanish: ghdirecto 4\n",
      "   swedish: svpol 4\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xc4 in position 3: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-3db3b29f23ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mhashtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m127\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhashtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"%10s: %s\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m', '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"%s %i\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTOP_N_HASHTAGS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xc4 in position 3: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "TOP_N_HASHTAGS = 1\n",
    "for lang in sorted(stopwords._fileids):\n",
    "    key = 'hashtag_' + lang\n",
    "    c = collections.Counter()\n",
    "    hashtags = r.lrange(key, -127, -1)\n",
    "    c.update(map(string.lower, hashtags))\n",
    "    print \"%10s: %s\"%(lang, ', '.join([\"%s %i\" % x for x in c.most_common(TOP_N_HASHTAGS)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total mentions of Yes: 0\n",
      "Percentage of all tweets: 0 %\n"
     ]
    }
   ],
   "source": [
    "#Checking for total mentions of 'Yes' during last 2 minutes\n",
    "if REALTIME_DATA:\n",
    "    TotalCount = 0\n",
    "    for i in range(10^7): #high number to ensure all tweets of last 2 minutes are checked\n",
    "        T = Tweet(stream.next())\n",
    "        if T:\n",
    "            T['language'] = get_likely_language(T['text'])[0]\n",
    "            Count = len(re.findall(('debate'),T['text']))\n",
    "            TotalCount = TotalCount + Count\n",
    "    print \"Total mentions of Yes:\",TotalCount\n",
    "    print \"Percentage of all tweets:\",TotalCount/200*100,\"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Checking for total mentions of 'Bye' during last 2 minutes, using window size of 300 tweets\n",
    "if REALTIME_DATA:\n",
    "    TotalCount = 0\n",
    "    for j in range(10^7): #High number to ensure all tweets of last 2 minutes are checked\n",
    "        for i in range(300): #Window size of 300 tweets\n",
    "            T = Tweet(stream.next())\n",
    "            if T:\n",
    "                T['language'] = get_likely_language(T['text'])[0]\n",
    "    #             print \"%s, %i, %i: %s\" % (get_likely_language(T['text']) + (T['text'],))\n",
    "                Count = len(re.findall(('bye'),T['text']))\n",
    "                TotalCount = TotalCount + Count\n",
    "    print \"Total mentions of Bye:\",TotalCount\n",
    "    print \"Percentage of all tweets:\",TotalCount/200*100,\"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
